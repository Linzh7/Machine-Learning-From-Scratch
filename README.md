# Machine-Learning-From-Scratch

## 线性回归

### 代价函数

无论使用哪种方式去寻找我们需要的线性方程，总要有一个评价指标。我们称之为代价函数，

### 不好的寻找方式：枚举

假设我们使用一个2层、每层10个节点的全连接网络，除bias外的参数就有100个，我们自然也可以找到一个「恰当」的参数，但其复杂度非常高。

但我们还是尝试去实现它，但是作为一个反面教材出现。

### 数学方法：最小二乘法

使用数学方法来拟合数据，结果可能不是最优，但其复杂度极低。

### 梯度下降
我们可以不断计算残差，然后找到更新每个参数，使其向最优方向再进化一点。反复迭代，即接近最优接解（正常拟合时）。

而对于批量梯度下降，我们是使用所有的样本来计算残差，因而在大样本的时候会出现运行缓慢的问题。因而改进算法，先随机打乱数据，然后从中选取任一来优化参数。这样虽然可能在局部出现非最优优化、甚至是劣化的现象，但整体上是向最优方向靠近的。

为了使进化方向更稳定，再优化算法如下。在打乱后，随机抽取指定数量的样本，以避免单个样本可能出现的离群值问题。


## 逻辑回归
在二分类逻辑回归中，我们通常使用Sigmoid函数来将预测结果缩小到(0,1)的范围内。

## 距离函数

### 欧几里德距离（欧氏距离）
与初中学的距离概念相同，即直接使用两点间线段的长度代表距离。

### 曼哈顿距离
使用坐标系下，与坐标轴平行的线段长度之和表示距离。所有线段首尾相连，从一点出发，在平行与坐标轴的前提下取得最短距离，连接到另一点。

### 切比雪夫距离
为其各座标数值差的最大值，字面意思。

### 闵可夫斯基距离（闵氏距离）
可以看作以上几种距离的集合，其维度等于1时，为曼哈顿距离；为2时，为欧几里德距离；大于2时，为切比雪夫距离。

