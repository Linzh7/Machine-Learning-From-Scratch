# Machine-Learning-From-Scratch

## 前言

个人的身体状况也不知道能不能支持写完这么多东西，但总会尽力去写。

因为是非科班出身，因而确实踩了很多坑，我尽力在完善这个项目的过程中将之一一说明（如果还记得的话）

## 线性回归

### 代价函数

无论使用哪种方式去寻找我们需要的线性方程，总要有一个评价指标。我们称之为代价函数，

### 不好的寻找方式：枚举

假设我们使用一个2层、每层10个节点的全连接网络，除bias外的参数就有100个，我们自然也可以找到一个「恰当」的参数，但其复杂度非常高。

但我们还是尝试去实现它，但是作为一个反面教材出现。

### 数学方法：最小二乘法

使用数学方法来拟合数据，结果可能不是最优，但其复杂度极低。

### 梯度下降
我们可以不断计算残差，然后找到更新每个参数，使其向最优方向再进化一点。反复迭代，即接近最优接解（正常拟合时）。

而对于批量梯度下降，我们是使用所有的样本来计算残差，因而在大样本的时候会出现运行缓慢的问题。因而改进算法，先随机打乱数据，然后从中选取任一来优化参数。这样虽然可能在局部出现非最优优化、甚至是劣化的现象，但整体上是向最优方向靠近的。

为了使进化方向更稳定，再优化算法如下。在打乱后，随机抽取指定数量的样本，以避免单个样本可能出现的离群值问题。