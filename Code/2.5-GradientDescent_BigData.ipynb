{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('anaconda3')",
   "display_name": "Python 3.8.3 64-bit ('anaconda3')",
   "metadata": {
    "interpreter": {
     "hash": "60dd2710664135357c468eb33a9da7f3dd8e597d11a4ea2c0d7126bd8a0e7f69"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Cost\n",
    "import DataReader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchGradientDescent(data_x,\n",
    "                         data_y,\n",
    "                         variableNum,\n",
    "                         epsilon=0.01,\n",
    "                         learningRate=0.0001):\n",
    "    count = 0\n",
    "    theta = np.ones((variableNum, ), dtype=np.float32)\n",
    "    predict_y = np.zeros((len(data_y), ), dtype=np.float32)\n",
    "    predictCost1 = -99999\n",
    "    while True:\n",
    "        time_start=time.time()\n",
    "        count += 1\n",
    "        diff = np.zeros((variableNum, ), dtype=np.float32)\n",
    "        \n",
    "        for index in range(len(data_y)):\n",
    "            predict_y[index] = np.dot(theta.T, data_x[index])\n",
    "            #print(theta.T, data_x[index], predict_y[index])\n",
    "            for i in range(variableNum):\n",
    "                diff[i] += (predict_y[index] -\n",
    "                            data_y[index]) * data_x[index][i]\n",
    "\n",
    "        for i in range(variableNum):\n",
    "            theta[i] -= learningRate * diff[i] / len(data_y)\n",
    "\n",
    "        predictCost0 = Cost.SquaredErrors(data_y, predict_y)\n",
    "        if abs(predictCost0 - predictCost1) < epsilon:\n",
    "            return theta, predictCost0\n",
    "        else:\n",
    "            predictCost1 = predictCost0\n",
    "        time_end=time.time()\n",
    "        if (count % 10 == 0):\n",
    "            print(count,':',time_end-time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def StochasticGradientDescent(data_x,\n",
    "                              data_y,\n",
    "                              variableNum,\n",
    "                              epsilon=0.01,\n",
    "                              learningRate=0.001):\n",
    "    count = 0\n",
    "    theta = np.ones((variableNum, ), dtype=np.float32)\n",
    "    predict_y = np.zeros((len(data_y), ), dtype=np.float32)\n",
    "    predictCost1 = -99999\n",
    "    while True:\n",
    "        count += 1\n",
    "        for index in range(len(data_y)):\n",
    "            diff = np.zeros((variableNum, ), dtype=np.float32)\n",
    "            predict_y[index] = np.dot(theta.T, data_x[index])\n",
    "            for i in range(variableNum):\n",
    "                diff[i] += (predict_y[index] -\n",
    "                            data_y[index]) * data_x[index][i]\n",
    "            for i in range(variableNum):\n",
    "                theta[i] -= learningRate * diff[i] / len(data_y)\n",
    "            predictCost0 = Cost.SquaredErrors(data_y, predict_y)\n",
    "            if abs(predictCost0 - predictCost1) < epsilon:\n",
    "                return theta, predictCost0\n",
    "            else:\n",
    "                predictCost1 = predictCost0\n",
    "\n",
    "def MinibatchGradientDescent(data_x,\n",
    "                                data_y,\n",
    "                                variableNum,\n",
    "                                batchSize,\n",
    "                                epsilon=0.01,\n",
    "                                learningRate=0.001):\n",
    "    # initialize\n",
    "    count = 0\n",
    "    theta = np.ones((variableNum, ), dtype=np.float32)\n",
    "    predict_y = np.zeros((len(data_y), ), dtype=np.float32)\n",
    "    predictCost1 = -99999\n",
    "\n",
    "    # operate\n",
    "    while True:\n",
    "        count += 1\n",
    "        diff = np.zeros((variableNum, ), dtype=np.float32)\n",
    "\n",
    "        # shuffle the data set\n",
    "        state = np.random.get_state()\n",
    "        np.random.shuffle(data_x)\n",
    "        np.random.set_state(state)\n",
    "        np.random.shuffle(data_y)\n",
    "\n",
    "        # compute diff\n",
    "        for index in range(batchSize):\n",
    "            predict_y[index] = np.dot(data_x[:,index], theta.T)\n",
    "            for i in range(variableNum):\n",
    "                diff[i] += (predict_y[index] -\n",
    "                            data_y[index]) * data_x[index][i]\n",
    "\n",
    "        # update theta\n",
    "        for i in range(variableNum):\n",
    "            theta[i] -= learningRate * diff[i] / len(data_y)\n",
    "\n",
    "        # stop condition\n",
    "        predict_y = np.dot(data_x, theta.T)\n",
    "        predictCost0 = Cost.SquaredErrors(data_y, predict_y)\n",
    "        if abs(predictCost0 - predictCost1) < epsilon:\n",
    "            return theta, predictCost0\n",
    "        else:\n",
    "            predictCost1 = predictCost0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in range(1,10000001):\n",
    "    a.append([i, 2 * i])\n",
    "data_x = np.array(a)\n",
    "data_y = data_x[0] * 3 + data_x[1] * 5 + np.random.normal() * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[       1,        2],\n       [       2,        4],\n       [       3,        6],\n       ...,\n       [ 9999998, 19999996],\n       [ 9999999, 19999998],\n       [10000000, 20000000]])"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "a = np.arange(1.0, 10000001.0,1.0)\n",
    "b = np.arange(2.0, 20000002.0, 2)\n",
    "\n",
    "#data_x = zeros([10000000, 2])\n",
    "data_x[:, 0] = a\n",
    "data_x[:, 1] = b\n",
    "data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10 : 0.00014209747314453125\n20 : 6.103515625e-05\n30 : 5.030632019042969e-05\n40 : 6.008148193359375e-05\n50 : 4.7206878662109375e-05\n60 : 6.508827209472656e-05\n70 : 5.3882598876953125e-05\n80 : 5.984306335449219e-05\n90 : 5.984306335449219e-05\n100 : 0.00010609626770019531\n110 : 6.318092346191406e-05\n120 : 5.602836608886719e-05\n130 : 4.792213439941406e-05\n140 : 5.030632019042969e-05\n150 : 6.008148193359375e-05\n160 : 5.698204040527344e-05\n170 : 5.1021575927734375e-05\n180 : 5.1021575927734375e-05\n190 : 4.7206878662109375e-05\n200 : 4.792213439941406e-05\n210 : 4.887580871582031e-05\n220 : 4.792213439941406e-05\n230 : 4.76837158203125e-05\n240 : 6.914138793945312e-05\n250 : 0.0001380443572998047\n260 : 4.7206878662109375e-05\n270 : 0.00010013580322265625\n280 : 6.890296936035156e-05\n290 : 0.00013399124145507812\n300 : 4.8160552978515625e-05\n310 : 6.198883056640625e-05\n320 : 0.00043392181396484375\n330 : 4.9114227294921875e-05\n340 : 7.605552673339844e-05\n350 : 6.079673767089844e-05\n360 : 5.221366882324219e-05\n370 : 4.601478576660156e-05\n380 : 4.57763671875e-05\n390 : 4.506111145019531e-05\n400 : 4.601478576660156e-05\n410 : 5.0067901611328125e-05\n420 : 4.696846008300781e-05\n430 : 4.6253204345703125e-05\n440 : 4.38690185546875e-05\n450 : 4.482269287109375e-05\n460 : 4.696846008300781e-05\n470 : 4.506111145019531e-05\n480 : 4.410743713378906e-05\n490 : 4.601478576660156e-05\n500 : 4.6253204345703125e-05\n510 : 4.506111145019531e-05\n520 : 4.982948303222656e-05\n530 : 4.482269287109375e-05\n540 : 4.601478576660156e-05\n550 : 4.601478576660156e-05\n560 : 4.506111145019531e-05\n570 : 4.601478576660156e-05\n580 : 4.6253204345703125e-05\n590 : 5.2928924560546875e-05\n600 : 5.1021575927734375e-05\n610 : 4.7206878662109375e-05\n620 : 4.76837158203125e-05\n630 : 4.7206878662109375e-05\n640 : 5.984306335449219e-05\n650 : 4.7206878662109375e-05\n660 : 5.2928924560546875e-05\n670 : 4.887580871582031e-05\n680 : 4.6253204345703125e-05\n690 : 4.601478576660156e-05\n700 : 4.7206878662109375e-05\n710 : 4.601478576660156e-05\n720 : 4.506111145019531e-05\n730 : 4.696846008300781e-05\n740 : 4.38690185546875e-05\n750 : 4.506111145019531e-05\n760 : 5.412101745605469e-05\n770 : 4.601478576660156e-05\n780 : 4.57763671875e-05\n790 : 4.601478576660156e-05\n800 : 5.507469177246094e-05\n810 : 4.982948303222656e-05\n820 : 6.389617919921875e-05\n830 : 4.887580871582031e-05\n840 : 4.887580871582031e-05\n850 : 0.00010228157043457031\n860 : 4.57763671875e-05\n870 : 4.601478576660156e-05\n880 : 4.673004150390625e-05\n890 : 4.506111145019531e-05\n900 : 4.506111145019531e-05\n910 : 4.601478576660156e-05\n920 : 4.57763671875e-05\n930 : 4.601478576660156e-05\n940 : 4.8160552978515625e-05\n950 : 4.601478576660156e-05\n960 : 4.601478576660156e-05\n970 : 4.506111145019531e-05\n980 : 4.458427429199219e-05\n990 : 4.482269287109375e-05\n1000 : 4.696846008300781e-05\n1010 : 4.506111145019531e-05\n1020 : 4.506111145019531e-05\n1030 : 7.104873657226562e-05\n1040 : 4.7206878662109375e-05\n1050 : 4.673004150390625e-05\n1060 : 0.0001399517059326172\n1070 : 4.982948303222656e-05\n1080 : 5.316734313964844e-05\n1090 : 6.914138793945312e-05\n1100 : 4.506111145019531e-05\n1110 : 4.506111145019531e-05\n1120 : 4.506111145019531e-05\n1130 : 4.57763671875e-05\n1140 : 4.506111145019531e-05\n1150 : 4.57763671875e-05\n1160 : 4.506111145019531e-05\n1170 : 4.506111145019531e-05\n1180 : 4.57763671875e-05\n1190 : 4.601478576660156e-05\n1200 : 4.6253204345703125e-05\n1210 : 4.601478576660156e-05\n1220 : 4.482269287109375e-05\n1230 : 4.482269287109375e-05\n1240 : 4.506111145019531e-05\n[2.345037  3.6900673] 4.2975381167338\nTime Cost: 0.1002049446105957\n"
    }
   ],
   "source": [
    "import time\n",
    "time_start=time.time()\n",
    "\n",
    "theta, cost = BatchGradientDescent(data_x, data_y, 2)\n",
    "print(theta, cost)\n",
    "\n",
    "time_end=time.time()\n",
    "\n",
    "print('Time Cost:',time_end-time_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.995741 4.991482 11.951501848614408\n",
    "\n",
    "Time Cost: 0.06s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[2.9962835 4.992566 ] 8.595104151832285\nTime Cost: 0.029675006866455078\n"
    }
   ],
   "source": [
    "import time\n",
    "time_start=time.time()\n",
    "\n",
    "theta, cost = StochasticGradientDescent(data_x, data_y, 2)\n",
    "print(theta, cost)\n",
    "\n",
    "time_end=time.time()\n",
    "\n",
    "print('Time Cost:',time_end-time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[2.995716 4.991432] 11.95276673753416\nTime Cost: 0.008604764938354492\n"
    }
   ],
   "source": [
    "import time\n",
    "time_start=time.time()\n",
    "\n",
    "theta, cost = MinibatchGradientDescent(data_x, data_y, 2, 2)\n",
    "print(theta, cost)\n",
    "\n",
    "time_end=time.time()\n",
    "\n",
    "print('Time Cost:',time_end-time_start)"
   ]
  }
 ]
}